{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12799368,"sourceType":"datasetVersion","datasetId":8086800}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-23T00:52:10.156766Z","iopub.execute_input":"2025-08-23T00:52:10.157056Z","iopub.status.idle":"2025-08-23T00:52:10.444646Z","shell.execute_reply.started":"2025-08-23T00:52:10.157034Z","shell.execute_reply":"2025-08-23T00:52:10.444075Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/pcamv1/camelyonpatch_level_2_split_train_y.h5\n/kaggle/input/pcamv1/camelyonpatch_level_2_split_valid_y.h5\n/kaggle/input/pcamv1/camelyonpatch_level_2_split_valid_meta.csv\n/kaggle/input/pcamv1/camelyonpatch_level_2_split_valid_x.h5\n/kaggle/input/pcamv1/camelyonpatch_level_2_split_train_mask.h5\n/kaggle/input/pcamv1/camelyonpatch_level_2_split_train_meta.csv\n/kaggle/input/pcamv1/camelyonpatch_level_2_split_test_y.h5\n/kaggle/input/pcamv1/camelyonpatch_level_2_split_test_meta.csv\n/kaggle/input/pcamv1/camelyonpatch_level_2_split_test_x.h5\n/kaggle/input/pcamv1/camelyonpatch_level_2_split_train_x.h5-001/camelyonpatch_level_2_split_train_x.h5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ===== 0) Imports & Setup =====\nimport os\nimport math\nimport h5py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_fscore_support\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom tqdm import tqdm\nimport torch.cuda.amp as amp  # Modified: Use cuda.amp for mixed precision\n\n# Reproducibility\nSEED = 1131\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Paths from Kaggle environment (updated for standard PCam dataset)\nBASE_DIR = \"/kaggle/input/pcamv1/\"\nTRAIN_X_PATH = os.path.join(BASE_DIR, \"camelyonpatch_level_2_split_train_x.h5-001/camelyonpatch_level_2_split_train_x.h5\")\nTRAIN_Y_PATH = os.path.join(BASE_DIR, \"camelyonpatch_level_2_split_train_y.h5\")\nVALID_X_PATH = os.path.join(BASE_DIR, \"camelyonpatch_level_2_split_valid_x.h5\")\nVALID_Y_PATH = os.path.join(BASE_DIR, \"camelyonpatch_level_2_split_valid_y.h5\")\nTEST_X_PATH = os.path.join(BASE_DIR, \"camelyonpatch_level_2_split_test_x.h5\")\nTEST_Y_PATH = os.path.join(BASE_DIR, \"camelyonpatch_level_2_split_test_y.h5\")\n\n# Verify paths\nfor path in [TRAIN_X_PATH, TRAIN_Y_PATH, VALID_X_PATH, VALID_Y_PATH, TEST_X_PATH, TEST_Y_PATH]:\n    if not os.path.exists(path):\n        print(f\"File not found: {path}\")\n    else:\n        print(f\"File found: {path}\")\n\n# Training hyperparameters\nINPUT_SIZE = 128  # Paper uses 128x128\nBATCH_SIZE = 64  # Modified: Increased back to 128 assuming GPU allows\nWARMUP_EPOCHS = 3\nFINETUNE_EPOCHS = 10  # Modified: Reduced from 12 to avoid overfitting\nLR_WARMUP = 1e-3\nLR_FINETUNE = 1e-4\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\n# Mixed precision\nMIXED_PRECISION = True\nprint(\"Mixed precision enabled\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T00:52:14.180425Z","iopub.execute_input":"2025-08-23T00:52:14.181156Z","iopub.status.idle":"2025-08-23T00:52:22.178971Z","shell.execute_reply.started":"2025-08-23T00:52:14.181130Z","shell.execute_reply":"2025-08-23T00:52:22.178159Z"}},"outputs":[{"name":"stdout","text":"File found: /kaggle/input/pcamv1/camelyonpatch_level_2_split_train_x.h5-001/camelyonpatch_level_2_split_train_x.h5\nFile found: /kaggle/input/pcamv1/camelyonpatch_level_2_split_train_y.h5\nFile found: /kaggle/input/pcamv1/camelyonpatch_level_2_split_valid_x.h5\nFile found: /kaggle/input/pcamv1/camelyonpatch_level_2_split_valid_y.h5\nFile found: /kaggle/input/pcamv1/camelyonpatch_level_2_split_test_x.h5\nFile found: /kaggle/input/pcamv1/camelyonpatch_level_2_split_test_y.h5\nUsing device: cuda\nMixed precision enabled\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===== 1) HDF5 Dataset =====\nclass PCamH5Dataset(Dataset):\n    def __init__(self, x_path, y_path, transform=None):\n        self.x_file = h5py.File(x_path, \"r\")\n        self.y_file = h5py.File(y_path, \"r\")\n        self.X = self.x_file[\"x\"]  # (N, 96, 96, 3) uint8\n        self.Y = self.y_file[\"y\"]  # (N, 1, 1, 1)\n        self.transform = transform\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        img = self.X[idx].astype(np.float32) / 255.0  # Normalize to [0,1]\n        label = float(self.Y[idx].reshape(-1)[0])\n        img = torch.from_numpy(img).permute(2, 0, 1)  # (3, 96, 96)\n        label = torch.tensor(label, dtype=torch.float32)\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n    def close(self):\n        self.x_file.close()\n        self.y_file.close()\n\n# Data augmentation (Modified: Added ColorJitter and Normalize)\ntrain_transform = transforms.Compose([\n    transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), shear=15),\n    transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.85, 1.0)),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nvalid_transform = transforms.Compose([\n    transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create datasets\ntrain_dataset = PCamH5Dataset(TRAIN_X_PATH, TRAIN_Y_PATH, transform=train_transform)\nvalid_dataset = PCamH5Dataset(VALID_X_PATH, VALID_Y_PATH, transform=valid_transform)\ntest_dataset = PCamH5Dataset(TEST_X_PATH, TEST_Y_PATH, transform=valid_transform)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\n# Compute class weights (Modified: Slight emphasis on positive class)\ndef count_class_distribution(y_path, chunk=65536):\n    f = h5py.File(y_path, \"r\")\n    Y = f[\"y\"]\n    n = Y.shape[0]\n    ones = 0\n    for start in range(0, n, chunk):\n        end = min(start + chunk, n)\n        ones += Y[start:end].reshape(-1).sum()\n    zeros = n - int(ones)\n    f.close()\n    return zeros, int(ones)\n\nneg, pos = count_class_distribution(TRAIN_Y_PATH)\nprint(f\"Train label counts → 0: {neg}, 1: {pos}\")\nclasses = np.array([0, 1])\nweights = [1.0, 1.2]  # Modified: Emphasize positive slightly\nclass_weight_dict = {0: weights[0], 1: weights[1]}\nprint(\"Class weights:\", class_weight_dict)\n\nclass_weights_tensor = torch.tensor([class_weight_dict[0], class_weight_dict[1]], dtype=torch.float32).to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T00:52:30.812738Z","iopub.execute_input":"2025-08-23T00:52:30.813042Z","iopub.status.idle":"2025-08-23T00:52:30.832389Z","shell.execute_reply.started":"2025-08-23T00:52:30.813021Z","shell.execute_reply":"2025-08-23T00:52:30.831697Z"}},"outputs":[{"name":"stdout","text":"Train label counts → 0: 131072, 1: 131072\nClass weights: {0: 1.0, 1: 1.2}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ===== 2) Model Definition (Modified: Enhanced classifier with extra layer) =====\nclass DNBCD(nn.Module):\n    def __init__(self):\n        super(DNBCD, self).__init__()\n        self.backbone = models.densenet121(pretrained=True)\n        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])  # Remove classifier\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.dropout1 = nn.Dropout(0.4)  # Modified: Increased dropout\n        self.fc1 = nn.Linear(1024, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.relu1 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.relu2 = nn.ReLU()\n        self.dropout3 = nn.Dropout(0.3)\n        self.fc3 = nn.Linear(256, 1)  # Output raw logits\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout1(x)\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x = self.dropout3(x)\n        x = self.fc3(x)\n        return x\n\n# Initialize model\nmodel = DNBCD().to(DEVICE)\n\n# Freeze backbone for warmup\nfor param in model.backbone.parameters():\n    param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T00:52:35.708301Z","iopub.execute_input":"2025-08-23T00:52:35.708894Z","iopub.status.idle":"2025-08-23T00:52:36.253722Z","shell.execute_reply.started":"2025-08-23T00:52:35.708870Z","shell.execute_reply":"2025-08-23T00:52:36.253039Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|██████████| 30.8M/30.8M [00:00<00:00, 156MB/s] \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ===== 3) Training Setup (Modified: FocalLoss, weight_decay, Cosine scheduler) =====\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        pt = torch.exp(-BCE_loss)  # prevents nans when probability 0\n        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n        if self.reduction == 'mean':\n            return torch.mean(F_loss)\n        elif self.reduction == 'sum':\n            return torch.sum(F_loss)\n        else:\n            return F_loss\n\ncriterion = FocalLoss(alpha=0.75, gamma=2.0)  # Modified: Use FocalLoss\n\noptimizer = optim.AdamW(model.parameters(), lr=LR_WARMUP, weight_decay=1e-4)  # Modified: Added weight_decay\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=FINETUNE_EPOCHS)  # Modified: Cosine scheduler for finetune\nscaler = amp.GradScaler()  # For mixed precision\nbest_auc = 0.0\nckpt_path = \"/kaggle/working/dnbcd_pcam_best.pth\"\n\n# Early stopping params (Modified: Added early stopping)\npatience = 3\nearly_stop_counter = 0\n\ndef train_epoch(loader, model, criterion, optimizer, scaler):\n    model.train()\n    total_loss, total_correct, total_samples = 0, 0, 0\n    all_probs, all_labels = [], []\n    \n    progress_bar = tqdm(loader, desc=\"Training\")\n    \n    for images, labels in progress_bar:\n        images, labels = images.to(DEVICE), labels.to(DEVICE).float()\n        \n        optimizer.zero_grad()\n        \n        with amp.autocast():\n            outputs = model(images).squeeze()\n            loss = criterion(outputs, labels)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        total_correct += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n        \n        all_probs.extend(torch.sigmoid(outputs).cpu().detach().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        \n        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    accuracy = total_correct / total_samples\n    auc = roc_auc_score(all_labels, all_probs)\n    avg_loss = total_loss / len(loader)\n    \n    return avg_loss, accuracy, auc\n\ndef validate_epoch(loader, model, criterion):\n    model.eval()\n    total_loss, total_correct, total_samples = 0, 0, 0\n    all_probs, all_labels, all_preds = [], [], []\n    \n    with torch.no_grad():\n        progress_bar = tqdm(loader, desc=\"Validation\")\n        \n        for images, labels in progress_bar:\n            images, labels = images.to(DEVICE), labels.to(DEVICE).float()\n            \n            with amp.autocast():\n                outputs = model(images).squeeze()\n                loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            predicted = (torch.sigmoid(outputs) > 0.5).float()  # Default threshold\n            total_correct += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n            \n            all_probs.extend(torch.sigmoid(outputs).cpu().detach().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n            \n            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    accuracy = total_correct / total_samples\n    auc = roc_auc_score(all_labels, all_probs)\n    avg_loss = total_loss / len(loader)\n    \n    # Modified: Add per-class metrics\n    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n    print(f\"Metastasis Precision: {prec[1]:.4f}, Recall: {rec[1]:.4f}, F1: {f1[1]:.4f}\")\n    \n    return avg_loss, accuracy, auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T00:52:41.405107Z","iopub.execute_input":"2025-08-23T00:52:41.405370Z","iopub.status.idle":"2025-08-23T00:52:41.420775Z","shell.execute_reply.started":"2025-08-23T00:52:41.405351Z","shell.execute_reply":"2025-08-23T00:52:41.420011Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/503217744.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = amp.GradScaler()  # For mixed precision\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ===== 4) Training Loop (Modified: Added early stopping, scheduler step) =====\nprint(\"Starting warm-up training phase...\")\nhistory = {'train_loss': [], 'train_acc': [], 'train_auc': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n\nfor epoch in range(WARMUP_EPOCHS):\n    train_loss, train_acc, train_auc = train_epoch(train_loader, model, criterion, optimizer, scaler)\n    val_loss, val_acc, val_auc = validate_epoch(valid_loader, model, criterion)\n    \n    print(f\"Warmup Epoch {epoch+1}/{WARMUP_EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}\")\n    \n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['train_auc'].append(train_auc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['val_auc'].append(val_auc)\n\n# Unfreeze backbone for finetuning\nfor param in model.backbone.parameters():\n    param.requires_grad = True\n\n# Reset optimizer for finetuning with new LR and higher weight_decay\noptimizer = optim.AdamW(model.parameters(), lr=LR_FINETUNE, weight_decay=1e-3)  # Modified: Higher weight_decay\n\nprint(\"Starting fine-tuning phase...\")\nfor epoch in range(FINETUNE_EPOCHS):\n    train_loss, train_acc, train_auc = train_epoch(train_loader, model, criterion, optimizer, scaler)\n    val_loss, val_acc, val_auc = validate_epoch(valid_loader, model, criterion)\n    \n    scheduler.step()  # Modified: Step the cosine scheduler\n    \n    print(f\"Finetune Epoch {epoch+1}/{FINETUNE_EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}\")\n    \n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['train_auc'].append(train_auc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['val_auc'].append(val_auc)\n    \n    # Save best model\n    if val_auc > best_auc:\n        best_auc = val_auc\n        torch.save(model.state_dict(), ckpt_path)\n        print(\"Saved best model\")\n        early_stop_counter = 0\n    else:\n        early_stop_counter += 1\n        if early_stop_counter >= patience:\n            print(\"Early stopping triggered\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T18:14:24.892000Z","iopub.execute_input":"2025-08-22T18:14:24.892349Z","iopub.status.idle":"2025-08-22T23:29:16.620427Z","shell.execute_reply.started":"2025-08-22T18:14:24.892321Z","shell.execute_reply":"2025-08-22T23:29:16.619446Z"}},"outputs":[{"name":"stdout","text":"Starting warm-up training phase...\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/4096 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nTraining: 100%|██████████| 4096/4096 [34:35<00:00,  1.97it/s, loss=0.0986]\nValidation:   0%|          | 0/512 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nValidation: 100%|██████████| 512/512 [00:41<00:00, 12.41it/s, loss=0.0988]\n","output_type":"stream"},{"name":"stdout","text":"Metastasis Precision: 0.8810, Recall: 0.7157, F1: 0.7898\nWarmup Epoch 1/3\nTrain Loss: 0.0821, Acc: 0.8067, AUC: 0.8877\nVal Loss: 0.0778, Acc: 0.8097, AUC: 0.9074\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/4096 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nTraining: 100%|██████████| 4096/4096 [33:03<00:00,  2.07it/s, loss=0.1147]\nValidation:   0%|          | 0/512 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nValidation: 100%|██████████| 512/512 [00:35<00:00, 14.60it/s, loss=0.0909]\n","output_type":"stream"},{"name":"stdout","text":"Metastasis Precision: 0.8754, Recall: 0.7251, F1: 0.7932\nWarmup Epoch 2/3\nTrain Loss: 0.0765, Acc: 0.8223, AUC: 0.9039\nVal Loss: 0.0765, Acc: 0.8111, AUC: 0.9094\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/4096 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nTraining: 100%|██████████| 4096/4096 [30:53<00:00,  2.21it/s, loss=0.0790]\nValidation:   0%|          | 0/512 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nValidation: 100%|██████████| 512/512 [00:35<00:00, 14.63it/s, loss=0.0841]\n","output_type":"stream"},{"name":"stdout","text":"Metastasis Precision: 0.8726, Recall: 0.7394, F1: 0.8005\nWarmup Epoch 3/3\nTrain Loss: 0.0747, Acc: 0.8273, AUC: 0.9087\nVal Loss: 0.0744, Acc: 0.8159, AUC: 0.9133\nStarting fine-tuning phase...\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/4096 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nTraining: 100%|██████████| 4096/4096 [34:53<00:00,  1.96it/s, loss=0.0515]\nValidation:   0%|          | 0/512 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nValidation: 100%|██████████| 512/512 [00:34<00:00, 14.66it/s, loss=0.1074]\n","output_type":"stream"},{"name":"stdout","text":"Metastasis Precision: 0.9584, Recall: 0.7705, F1: 0.8542\nFinetune Epoch 1/10\nTrain Loss: 0.0438, Acc: 0.9135, AUC: 0.9702\nVal Loss: 0.0680, Acc: 0.8687, AUC: 0.9592\nSaved best model\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/4096 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nTraining: 100%|██████████| 4096/4096 [34:15<00:00,  1.99it/s, loss=0.0251]\nValidation:   0%|          | 0/512 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nValidation: 100%|██████████| 512/512 [00:36<00:00, 14.00it/s, loss=0.1145]\n","output_type":"stream"},{"name":"stdout","text":"Metastasis Precision: 0.9632, Recall: 0.7572, F1: 0.8479\nFinetune Epoch 2/10\nTrain Loss: 0.0334, Acc: 0.9382, AUC: 0.9824\nVal Loss: 0.0848, Acc: 0.8643, AUC: 0.9534\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/4096 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nTraining: 100%|██████████| 4096/4096 [33:38<00:00,  2.03it/s, loss=0.0209]\nValidation:   0%|          | 0/512 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nValidation: 100%|██████████| 512/512 [00:35<00:00, 14.58it/s, loss=0.0856]\n","output_type":"stream"},{"name":"stdout","text":"Metastasis Precision: 0.9616, Recall: 0.8302, F1: 0.8911\nFinetune Epoch 3/10\nTrain Loss: 0.0295, Acc: 0.9468, AUC: 0.9862\nVal Loss: 0.0639, Acc: 0.8986, AUC: 0.9630\nSaved best model\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/4096 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nTraining: 100%|██████████| 4096/4096 [34:51<00:00,  1.96it/s, loss=0.0168]\nValidation:   0%|          | 0/512 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nValidation: 100%|██████████| 512/512 [00:35<00:00, 14.39it/s, loss=0.0968]\n","output_type":"stream"},{"name":"stdout","text":"Metastasis Precision: 0.9568, Recall: 0.8303, F1: 0.8891\nFinetune Epoch 4/10\nTrain Loss: 0.0269, Acc: 0.9522, AUC: 0.9884\nVal Loss: 0.0668, Acc: 0.8965, AUC: 0.9613\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/4096 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nTraining: 100%|██████████| 4096/4096 [36:40<00:00,  1.86it/s, loss=0.0272]\nValidation:   0%|          | 0/512 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nValidation: 100%|██████████| 512/512 [00:36<00:00, 13.85it/s, loss=0.1107]\n","output_type":"stream"},{"name":"stdout","text":"Metastasis Precision: 0.9530, Recall: 0.8184, F1: 0.8806\nFinetune Epoch 5/10\nTrain Loss: 0.0250, Acc: 0.9562, AUC: 0.9899\nVal Loss: 0.0672, Acc: 0.8891, AUC: 0.9587\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/4096 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nTraining: 100%|██████████| 4096/4096 [36:32<00:00,  1.87it/s, loss=0.0339]\nValidation:   0%|          | 0/512 [00:00<?, ?it/s]/tmp/ipykernel_36/503217744.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\nValidation: 100%|██████████| 512/512 [00:36<00:00, 13.94it/s, loss=0.1478]","output_type":"stream"},{"name":"stdout","text":"Metastasis Precision: 0.9544, Recall: 0.8027, F1: 0.8720\nFinetune Epoch 6/10\nTrain Loss: 0.0238, Acc: 0.9580, AUC: 0.9909\nVal Loss: 0.0729, Acc: 0.8823, AUC: 0.9577\nEarly stopping triggered\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ===== 5) Evaluation (Updated to match provided code) =====\nprint(\"Loading best model for evaluation...\")\nmodel.load_state_dict(torch.load(ckpt_path))\nmodel.eval()\n\n# Test evaluation\ntest_loss, test_acc, test_auc = validate_epoch(test_loader, model, criterion)\nprint(f\"Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n\n# Detailed metrics\nprint(\"Generating detailed predictions...\")\npreds, trues = [], []\nwith torch.no_grad():\n    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n        images = images.to(DEVICE)\n        \n        if MIXED_PRECISION:\n            with torch.cuda.amp.autocast():\n                outputs = model(images).squeeze()\n        else:\n            outputs = model(images).squeeze()\n        \n        preds.extend(torch.sigmoid(outputs).cpu().numpy())  # Apply sigmoid for metrics\n        trues.extend(labels.cpu().numpy())\n\n# Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(\n    trues, \n    (np.array(preds) > 0.5).astype(int), \n    target_names=['No Metastasis', 'Metastasis']\n))\n\n# Confusion Matrix\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(trues, (np.array(preds) > 0.5).astype(int))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['No Metastasis', 'Metastasis'], \n            yticklabels=['No Metastasis', 'Metastasis'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n# Plot training history\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history['train_acc'], label='Train Acc', color='blue')\nplt.plot(history['val_acc'], label='Val Acc', color='orange')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(history['train_loss'], label='Train Loss', color='blue')\nplt.plot(history['val_loss'], label='Val Loss', color='orange')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Additional metrics\nprint(f\"\\nAdditional Metrics:\")\nprint(f\"AUC-ROC Score: {roc_auc_score(trues, preds):.4f}\")\n\n# Threshold analysis\nthresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\nprint(f\"\\nThreshold Analysis:\")\nfor thresh in thresholds:\n    pred_binary = (np.array(preds) > thresh).astype(int)\n    acc = (pred_binary == np.array(trues)).mean()\n    prec, rec, f1, _ = precision_recall_fscore_support(trues, pred_binary, average=None)\n    print(f\"Threshold {thresh}: Accuracy = {acc:.4f}, Metastasis Recall = {rec[1]:.4f}\")\n\n# Clean up datasets\nprint(\"\\nCleaning up...\")\nif hasattr(train_dataset, 'close'):\n    train_dataset.close()\nif hasattr(valid_dataset, 'close'):\n    valid_dataset.close()\nif hasattr(test_dataset, 'close'):\n    test_dataset.close()\n\nprint(\"Training and evaluation completed!\")\n\n# Optional: Save final results\nresults_summary = {\n    'best_validation_auc': best_auc,\n    'test_loss': test_loss,\n    'test_accuracy': test_acc,\n    'test_auc': test_auc,\n    'training_history': history\n}\n\n# Save results to file\nwith open('/kaggle/working/training_results.json', 'w') as f:\n    json.dump(results_summary, f, indent=2)\n\nprint(\"Results saved to training_results.json\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-22T14:41:30.566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}